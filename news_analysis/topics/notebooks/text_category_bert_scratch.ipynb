{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "from farm.modeling.tokenization import Tokenizer\n",
    "from farm.data_handler.data_silo import StreamingDataSilo, DataSilo\n",
    "from farm.data_handler.processor import BertStyleLMProcessor\n",
    "from farm.data_handler.utils import split_file\n",
    "from farm.modeling.adaptive_model import AdaptiveModel\n",
    "from farm.modeling.language_model import LanguageModel\n",
    "from farm.modeling.optimization import initialize_optimizer\n",
    "from farm.modeling.prediction_head import BertLMHead, NextSentenceHead\n",
    "from farm.train import Trainer\n",
    "from farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings\n",
    "import torch\n",
    "\n",
    "# To get the best speed in a multi-GPU environment, launch the script via\n",
    "# python -m torch.distributed.launch --nproc_per_node=<NUM_GPUS> train_from_scratch.py\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--local_rank\",\n",
    "                        type=int,\n",
    "                        default=-1,\n",
    "                        help=\"local_rank for distributed training on GPUs\")\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def train_from_scratch():\n",
    "    use_amp = \"O2\"  # using \"O2\" here allows roughly 30% larger batch_sizes and 45% speed up\n",
    "\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "\n",
    "    set_all_seeds(seed=39)\n",
    "    device, n_gpu = initialize_device_settings(use_cuda=True)\n",
    "\n",
    "    save_dir = Path(\"saved_models/train_from_scratch\")\n",
    "    data_dir = Path(\".\")\n",
    "\n",
    "    # Option A) just using a single file\n",
    "    # train_filename = \"train.txt\"\n",
    "\n",
    "    # Option B) (recommended when using StreamingDataSilo):\n",
    "    # split and shuffle that file to have random order within and across epochs\n",
    "    split_file(data_dir / \"remain_conservative.txt\", output_dir=Path(\"data/split_files\"), docs_per_file=1000)\n",
    "    train_filename = Path(\"data/split_files\")\n",
    "\n",
    "    dev_filename = \"remain_labour.txt\"\n",
    "\n",
    "    distributed = False\n",
    "    max_seq_len = 128\n",
    "    batch_size = 8 #if distributed: this is per_gpu\n",
    "    grad_acc = 1\n",
    "    learning_rate = 1e-4\n",
    "    warmup_proportion = 0.05\n",
    "    n_epochs = 2\n",
    "    evaluate_every = 15000\n",
    "    log_loss_every=2\n",
    "    checkpoint_every = 500\n",
    "    checkpoint_root_dir = Path(\"checkpoints\")\n",
    "    checkpoints_to_keep = 4\n",
    "    next_sent_pred_style = \"bert-style\" #or \"sentence\"\n",
    "    max_docs = None\n",
    "\n",
    "    # Choose enough workers to queue sufficient batches during training.\n",
    "    # Optimal number depends on your GPU speed, CPU speed and number of cores\n",
    "    # 16 works well on a 4x V100 machine with 16 cores (AWS: p3.8xlarge). For a single GPU you will need less.\n",
    "    data_loader_workers = 1\n",
    "\n",
    "    # 1.Create a tokenizer\n",
    "    tokenizer = Tokenizer.load(\"bert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "    # 2. Create a DataProcessor that handles all the conversion from raw text into a PyTorch Dataset\n",
    "    processor = BertStyleLMProcessor(\n",
    "        data_dir=data_dir,\n",
    "        tokenizer=tokenizer, max_seq_len=max_seq_len,\n",
    "        train_filename=train_filename,\n",
    "        dev_filename=dev_filename,\n",
    "        test_filename=None,\n",
    "        next_sent_pred_style=next_sent_pred_style,\n",
    "        max_docs=max_docs\n",
    "    )\n",
    "    # 3. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and\n",
    "    #    calculates a few descriptive statistics of our datasets\n",
    "    # stream_data_silo = DataSilo(processor=processor, batch_size=batch_size, distributed=distributed)\n",
    "    stream_data_silo = StreamingDataSilo(processor=processor, batch_size=batch_size, distributed=distributed,\n",
    "                                         dataloader_workers=data_loader_workers)\n",
    "\n",
    "    # 4. Create an AdaptiveModel\n",
    "    # a) which consists of a pretrained language model as a basis\n",
    "    language_model = LanguageModel.from_scratch(\"bert\", tokenizer.vocab_size)\n",
    "\n",
    "    # b) and *two* prediction heads on top that are suited for our task => Language Model finetuning\n",
    "    lm_prediction_head = BertLMHead(768, tokenizer.vocab_size)\n",
    "    next_sentence_head = NextSentenceHead(num_labels=2, task_name=\"nextsentence\")\n",
    "\n",
    "    model = AdaptiveModel(\n",
    "        language_model=language_model,\n",
    "        prediction_heads=[lm_prediction_head, next_sentence_head],\n",
    "        embeds_dropout_prob=0.1,\n",
    "        lm_output_types=[\"per_token\", \"per_sequence\"],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # 5. Create an optimizer\n",
    "    model, optimizer, lr_schedule = initialize_optimizer(\n",
    "        model=model,\n",
    "        learning_rate=learning_rate,\n",
    "        schedule_opts={\"name\": \"LinearWarmup\", \"warmup_proportion\": warmup_proportion},\n",
    "        n_batches=len(stream_data_silo.get_data_loader(\"train\")),\n",
    "        n_epochs=n_epochs,\n",
    "        device=device,\n",
    "        grad_acc_steps=grad_acc,\n",
    "        distributed=distributed\n",
    "    )\n",
    "\n",
    "    # 6. Feed everything to the Trainer, which keeps care of growing our model and evaluates it from time to time\n",
    "    trainer = Trainer.create_or_load_checkpoint(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        data_silo=stream_data_silo,\n",
    "        epochs=n_epochs,\n",
    "        n_gpu=n_gpu,\n",
    "        lr_schedule=lr_schedule,\n",
    "        evaluate_every=evaluate_every,\n",
    "        log_loss_every=log_loss_every,\n",
    "        device=device,\n",
    "        grad_acc_steps=grad_acc,\n",
    "        checkpoint_every=checkpoint_every,\n",
    "        checkpoint_root_dir=checkpoint_root_dir,\n",
    "        checkpoints_to_keep=checkpoints_to_keep,\n",
    "    )\n",
    "    # 7. Let it grow! Watch the tracked metrics live on the public mlflow server: https://public-mlflow.deepset.ai\n",
    "    trainer.train()\n",
    "\n",
    "    # 8. Hooray! You have a model. Store it:\n",
    "    model.save(save_dir)\n",
    "    processor.save(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/07/2021 10:18:12 - INFO - farm.utils -   Using device: CPU \n",
      "06/07/2021 10:18:12 - INFO - farm.utils -   Number of GPUs: 0\n",
      "06/07/2021 10:18:12 - INFO - farm.utils -   Distributed Training: False\n",
      "06/07/2021 10:18:12 - INFO - farm.utils -   Automatic Mixed Precision: None\n",
      "Splitting file ...: 100%|██████████| 54176/54176 [00:00<00:00, 156533.29it/s]\n",
      "06/07/2021 10:18:12 - INFO - farm.data_handler.utils -   The input file remain_conservative.txt is split in 28 parts at data/split_files.\n",
      "06/07/2021 10:18:13 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'BertTokenizer'\n",
      "06/07/2021 10:18:18 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 2]\n",
      "06/07/2021 10:18:18 - INFO - root -   Estimating total number of samples ...\n",
      "06/07/2021 10:18:18 - INFO - farm.data_handler.utils -   Reached number of max_docs (500). Skipping rest of file ...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (754 > 512). Running this sequence through the model will result in indexing errors\n",
      "06/07/2021 10:18:18 - INFO - root -   Heuristic estimate of number of samples in data/split_files/part_23 based on 500 docs: 0\n",
      "06/07/2021 10:18:18 - INFO - farm.data_handler.data_silo -   Found data for 0 samples\n",
      "06/07/2021 10:18:18 - INFO - farm.modeling.optimization -   Loading optimizer `TransformersAdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 0.0001}'\n",
      "06/07/2021 10:18:18 - INFO - farm.modeling.optimization -   Using scheduler 'get_linear_schedule_with_warmup'\n",
      "06/07/2021 10:18:18 - INFO - farm.modeling.optimization -   Loading schedule `get_linear_schedule_with_warmup`: '{'num_training_steps': 0, 'num_warmup_steps': 0}'\n",
      "06/07/2021 10:18:19 - INFO - root -   No train checkpoints found. Starting a new training ...\n",
      "06/07/2021 10:18:19 - INFO - farm.train -   \n",
      " \n",
      "\n",
      "          &&& &&  & &&             _____                   _             \n",
      "      && &\\/&\\|& ()|/ @, &&       / ____|                 (_)            \n",
      "      &\\/(/&/&||/& /_/)_&/_&     | |  __ _ __ _____      ___ _ __   __ _ \n",
      "   &() &\\/&|()|/&\\/ '%\" & ()     | | |_ | '__/ _ \\ \\ /\\ / / | '_ \\ / _` |\n",
      "  &_\\_&&_\\ |& |&&/&__%_/_& &&    | |__| | | | (_) \\ V  V /| | | | | (_| |\n",
      "&&   && & &| &| /& & % ()& /&&    \\_____|_|  \\___/ \\_/\\_/ |_|_| |_|\\__, |\n",
      " ()&_---()&\\&\\|&&-&&--%---()~                                       __/ |\n",
      "     &&     \\|||                                                   |___/\n",
      "             |||\n",
      "             |||\n",
      "             |||\n",
      "       , -=-~  .-^- _\n",
      "              `\n",
      "\n",
      "06/07/2021 10:18:19 - INFO - root -   Estimating total number of samples ...\n",
      "06/07/2021 10:18:19 - INFO - farm.data_handler.utils -   Reached number of max_docs (500). Skipping rest of file ...\n",
      "06/07/2021 10:18:19 - INFO - root -   Heuristic estimate of number of samples in data/split_files/part_23 based on 500 docs: 0\n",
      "06/07/2021 10:18:19 - INFO - farm.data_handler.data_silo -   Found data for 0 samples\n",
      "Train epoch 0/1 (Cur. train loss: 0.0000): : 0it [00:00, ?it/s]06/07/2021 10:18:31 - INFO - farm.train -   Saving a train checkpoint ...\n",
      "/home/ubuntu/anaconda3/envs/econ/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "\nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-70746a2fc4a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_from_scratch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-55fe1968f48c>\u001b[0m in \u001b[0;36mtrain_from_scratch\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m     )\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# 7. Let it grow! Watch the tracked metrics live on the public mlflow server: https://public-mlflow.deepset.ai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;31m# 8. Hooray! You have a model. Store it:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FARM/farm/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_every\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_rank\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m                     \u001b[0;31m# Let other ranks wait until rank 0 has finished saving\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_rank\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FARM/farm/train.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m                 \u001b[0;34m\"numpy_rng_state\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m                 \u001b[0;34m\"rng_state\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_rng_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m                 \u001b[0;34m\"cuda_rng_state\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_rng_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m             },\n\u001b[1;32m    567\u001b[0m             \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"trainer\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/econ/lib/python3.7/site-packages/torch/cuda/random.py\u001b[0m in \u001b[0;36mget_rng_state\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mThis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0meagerly\u001b[0m \u001b[0minitializes\u001b[0m \u001b[0mCUDA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \"\"\"\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/econ/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m             raise RuntimeError(\n\u001b[1;32m    185\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             raise AssertionError(\n",
      "\u001b[0;32m~/anaconda3/envs/econ/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mFound\u001b[0m \u001b[0mno\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0mon\u001b[0m \u001b[0myour\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mGPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minstalled\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0;32mfrom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m http://www.nvidia.com/Download/index.aspx\"\"\")\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# TODO: directly link to the alternative bin that needs install\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "train_from_scratch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:econ] *",
   "language": "python",
   "name": "conda-env-econ-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
