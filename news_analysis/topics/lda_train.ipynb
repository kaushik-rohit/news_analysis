{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import models, corpora\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation\n",
    "from gensim.parsing.preprocessing import remove_stopwords, stem_text, strip_non_alphanum, strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import strip_short, strip_numeric\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import parmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_index_to_name_map = {\n",
    "    0: 'Agriculture, animals, food and rural affairs',\n",
    "    1: 'Asylum, immigration and nationality',\n",
    "    2: 'Business, industry and consumers',\n",
    "    3: 'Communities and families',\n",
    "    4: 'Crime, civil law, justice and rights',\n",
    "    5: 'Culture, media and sport',\n",
    "    6: 'Defence',\n",
    "    7: 'Economy and finance',\n",
    "    8: 'Education',\n",
    "    9: 'Employment and training',\n",
    "    10: 'Energy and environment',\n",
    "    11: 'European Union',\n",
    "    12: 'Health services and medicine',\n",
    "    13: 'Housing and planning',\n",
    "    14: 'International affairs',\n",
    "    15: 'Parliament, government and politics',\n",
    "    16: 'Science and technology',\n",
    "    17: 'Social security and pensions',\n",
    "    18: 'Social services',\n",
    "    19: 'Transport',\n",
    "    20: 'Others'\n",
    "}\n",
    "topics_name_to_index_map = {y:x for x,y in topics_index_to_name_map.items()}\n",
    "\n",
    "def strip_short2(text):\n",
    "    return strip_short(text, minsize=4)\n",
    "\n",
    "def remove_non_nouns(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    filter_tokens = [t[0] for t in tags if t[1] == \"NN\" or t[1] == \"VB\"]\n",
    "    return ' '.join(filter_tokens)\n",
    "\n",
    "\n",
    "def remove_custom_stopwords(s):\n",
    "    my_stop_words = STOPWORDS.union(set(['time', 'year', 'number', 'today', 'week', 'month', 'night', 'world', 'home',\n",
    "                                         'place', 'yesterday', 'life', 'wife']))\n",
    "    return \" \".join(w for w in s.split() if w not in my_stop_words)\n",
    "\n",
    "\n",
    "def preprocess_text_for_lda(text):\n",
    "    LDA_FILTERS = [lambda x: x.lower(), strip_multiple_whitespaces, strip_tags, strip_punctuation,\n",
    "                   remove_custom_stopwords, strip_short2, strip_non_alphanum, strip_numeric, remove_non_nouns]\n",
    "    return preprocess_string(text, LDA_FILTERS)\n",
    "\n",
    "def filter_multiple_topics(topic):\n",
    "    if '|' in topic:\n",
    "        return topic.split('|')[0].strip()\n",
    "    return topic\n",
    "\n",
    "def preprocess(topic):\n",
    "    t = filter_multiple_topics(topic)\n",
    "    if t == 'admin':\n",
    "        t = 'Others'\n",
    "        \n",
    "    return topics_name_to_index_map[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./data/2012_speech.csv')\n",
    "df2 = pd.read_csv('./data/2013_speech.csv')\n",
    "df = pd.concat([df1, df2])\n",
    "df = df.drop(['date'], axis=1)\n",
    "df = df.drop(df[df.topic == 'admin'].index)\n",
    "df = df.drop(df[df.transcript.str.split().map(len) < 10].index)\n",
    "df['topic'] = df.apply(lambda row: preprocess(row['topic']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7     11690\n",
       "15    11319\n",
       "20    10631\n",
       "4      8794\n",
       "14     6868\n",
       "9      5914\n",
       "12     5729\n",
       "3      5577\n",
       "2      4576\n",
       "19     4438\n",
       "11     3868\n",
       "6      3762\n",
       "10     3753\n",
       "8      3425\n",
       "5      3392\n",
       "0      2183\n",
       "13     1334\n",
       "1      1204\n",
       "17     1191\n",
       "16      549\n",
       "18      548\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['transcript'].values\n",
    "Y = df['topic'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100745 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "preprocessed_X = parmap.map(preprocess_text_for_lda, X, pm_pbar=True)\n",
    "#list(map(preprocess_text_for_lda, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create bigram and dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model = Phrases(preprocessed_X, min_count=1, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(bigram_model[preprocessed_X])\n",
    "dictionary.filter_extremes(no_above=0.40, no_below=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dictionary.save(os.path.join('.', 'topics_vocab_{}.dict'.format(\"2012-13\")))\n",
    "bigram = Phraser(bigram_model)\n",
    "bigram.save(os.path.join('.', \"bigram_{}.pkl\".format(\"2012-13\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert to bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100864it [00:05, 18042.48it/s]                           \n"
     ]
    }
   ],
   "source": [
    "#bow_X = list(map(dictionary.doc2bow, bigram_model[preprocessed_X]))\n",
    "preprocessed_X = bigram_model[preprocessed_X]\n",
    "bow_X = parmap.map(dictionary.doc2bow, preprocessed_X, pm_pbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "n_topics = 1000\n",
    "mallet_path = \"~/Mallet/bin/mallet\"\n",
    "\n",
    "model = models.wrappers.LdaMallet(mallet_path, corpus=bow_X, num_topics=n_topics, id2word=dictionary)\n",
    "coherencemodel = CoherenceModel(model=model, texts=bigram_model[preprocessed_X], dictionary=dictionary, coherence='c_v')\n",
    "lda_model = models.wrappers.ldamallet.malletmodel2ldamodel(model)\n",
    "lda_model.save('./lda_model_2012-13.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare inputs for NN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100864it [00:50, 1985.46it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (80596, 1000)\n",
      "y_train:  (80596, 21)\n",
      "X_test:  (20149, 1000)\n",
      "y_test:  (20149, 21)\n"
     ]
    }
   ],
   "source": [
    "def transform_to_lda_vector(bowx):\n",
    "    topics = lda_model.get_document_topics(bowx, minimum_probability=0.0)\n",
    "    topic_vec = [topics[i][1] for i in range(n_topics)]\n",
    "    return topic_vec\n",
    "\n",
    "inputs_X = parmap.map(transform_to_lda_vector, bow_X, pm_pbar=True)\n",
    "\n",
    "# for bowx in bow_X:\n",
    "#     topics = lda_model.get_document_topics(bowx, minimum_probability=0.0)\n",
    "#     topic_vec = [topics[i][1] for i in range(n_topics)]\n",
    "#     inputs.append(topic_vec)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "inputs_Y = Y.reshape(-1,1)\n",
    "enc.fit(inputs_Y)\n",
    "inputs_Y = enc.transform(inputs_Y).toarray()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs_X, inputs_Y, test_size=0.2, stratify=inputs_Y, random_state=42)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "print('X_train: ', X_train.shape)\n",
    "print('y_train: ', y_train.shape)\n",
    "print('X_test: ', X_test.shape)\n",
    "print('y_test: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.Input(shape=(n_topics)),\n",
    "        tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l=0.00001)),\n",
    "        tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l=0.00001)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(21, activation='softmax')\n",
    "    ])\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=0, name='categorical_crossentropy')\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80596 samples, validate on 20149 samples\n",
      "Epoch 1/200\n",
      "80596/80596 [==============================] - 12s 149us/sample - loss: 2.3694 - accuracy: 0.2905 - val_loss: 2.1507 - val_accuracy: 0.3829\n",
      "Epoch 2/200\n",
      "80596/80596 [==============================] - 10s 128us/sample - loss: 2.0462 - accuracy: 0.4060 - val_loss: 2.0804 - val_accuracy: 0.4044\n",
      "Epoch 3/200\n",
      "80596/80596 [==============================] - 11s 138us/sample - loss: 1.9866 - accuracy: 0.4206 - val_loss: 2.0203 - val_accuracy: 0.4176\n",
      "Epoch 4/200\n",
      "80596/80596 [==============================] - 11s 136us/sample - loss: 1.9521 - accuracy: 0.4298 - val_loss: 2.0412 - val_accuracy: 0.4107\n",
      "Epoch 5/200\n",
      "80596/80596 [==============================] - 19s 232us/sample - loss: 1.9233 - accuracy: 0.4373 - val_loss: 1.9939 - val_accuracy: 0.4246\n",
      "Epoch 6/200\n",
      "80596/80596 [==============================] - 19s 239us/sample - loss: 1.8996 - accuracy: 0.4453 - val_loss: 2.0299 - val_accuracy: 0.4115\n",
      "Epoch 7/200\n",
      "80596/80596 [==============================] - 18s 223us/sample - loss: 1.8767 - accuracy: 0.4501 - val_loss: 1.9949 - val_accuracy: 0.4283\n",
      "Epoch 8/200\n",
      "80596/80596 [==============================] - 13s 162us/sample - loss: 1.8533 - accuracy: 0.4584 - val_loss: 2.0214 - val_accuracy: 0.4185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fab1ca0f410>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_network()\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=200, callbacks=[callback], validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./lda_topics_classifier')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:econ] *",
   "language": "python",
   "name": "conda-env-econ-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
