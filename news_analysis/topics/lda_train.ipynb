{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohit/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rohit/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rohit/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rohit/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rohit/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rohit/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim import models, corpora\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation\n",
    "from gensim.parsing.preprocessing import remove_stopwords, stem_text, strip_non_alphanum, strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import strip_short, strip_numeric\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_index_to_name_map = {\n",
    "    0: 'Agriculture, animals, food and rural affairs',\n",
    "    1: 'Asylum, immigration and nationality',\n",
    "    2: 'Business, industry and consumers',\n",
    "    3: 'Communities and families',\n",
    "    4: 'Crime, civil law, justice and rights',\n",
    "    5: 'Culture, media and sport',\n",
    "    6: 'Defence',\n",
    "    7: 'Economy and finance',\n",
    "    8: 'Education',\n",
    "    9: 'Employment and training',\n",
    "    10: 'Energy and environment',\n",
    "    11: 'European Union',\n",
    "    12: 'Health services and medicine',\n",
    "    13: 'Housing and planning',\n",
    "    14: 'International affairs',\n",
    "    15: 'Parliament, government and politics',\n",
    "    16: 'Science and technology',\n",
    "    17: 'Social security and pensions',\n",
    "    18: 'Social services',\n",
    "    19: 'Transport',\n",
    "    20: 'Others'\n",
    "}\n",
    "topics_name_to_index_map = {y:x for x,y in topics_index_to_name_map.items()}\n",
    "\n",
    "def strip_short2(text):\n",
    "    return strip_short(text, minsize=4)\n",
    "\n",
    "def remove_non_nouns(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    filter_tokens = [t[0] for t in tags if t[1] == \"NN\" or t[1] == \"VB\"]\n",
    "    return ' '.join(filter_tokens)\n",
    "\n",
    "\n",
    "def remove_custom_stopwords(s):\n",
    "    my_stop_words = STOPWORDS.union(set(['time', 'year', 'number', 'today', 'week', 'month', 'night', 'world', 'home',\n",
    "                                         'place', 'yesterday', 'life', 'wife']))\n",
    "    return \" \".join(w for w in s.split() if w not in my_stop_words)\n",
    "\n",
    "\n",
    "def preprocess_text_for_lda(text):\n",
    "    LDA_FILTERS = [lambda x: x.lower(), strip_multiple_whitespaces, strip_tags, strip_punctuation,\n",
    "                   remove_custom_stopwords, strip_short2, strip_non_alphanum, strip_numeric, remove_non_nouns]\n",
    "    return preprocess_string(text, LDA_FILTERS)\n",
    "\n",
    "def filter_multiple_topics(topic):\n",
    "    if '|' in topic:\n",
    "        return topic.split('|')[0].strip()\n",
    "    return topic\n",
    "\n",
    "def preprocess(topic):\n",
    "    t = filter_multiple_topics(topic)\n",
    "    if t == 'admin':\n",
    "        t = 'Others'\n",
    "        \n",
    "    return topics_name_to_index_map[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/2013_speech.csv')\n",
    "df = df.drop(df[df.topic == 'admin'].index)\n",
    "df['topic'] = df.apply(lambda row: preprocess(row['topic']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.append([df[df.topic == 18]]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15    7457\n",
       "7     6713\n",
       "20    6640\n",
       "4     5532\n",
       "14    4585\n",
       "12    3531\n",
       "9     3142\n",
       "3     3030\n",
       "2     3019\n",
       "11    2942\n",
       "19    2843\n",
       "6     2442\n",
       "10    2304\n",
       "8     2233\n",
       "5     1845\n",
       "0     1619\n",
       "13    1387\n",
       "17    1043\n",
       "1     1006\n",
       "16     364\n",
       "18     188\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['transcript'].values\n",
    "Y = df['topic'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_X = list(map(preprocess_text_for_lda, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model = Phrases(preprocessed_X, min_count=1, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(bigram_model[preprocessed_X])\n",
    "dictionary.filter_extremes(no_above=0.40, no_below=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dictionary.save(os.path.join('.', 'topics_vocab_{}.dict'.format(2015)))\n",
    "bigram = Phraser(bigram_model)\n",
    "bigram.save(os.path.join('.', \"bigram_{}.pkl\".format(2015)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_X = list(map(dictionary.doc2bow, bigram_model[preprocessed_X]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "n_topics = 400\n",
    "mallet_path = \"/home/rohit/Mallet/bin/mallet\"\n",
    "\n",
    "model = models.wrappers.LdaMallet(mallet_path, corpus=bow_X, num_topics=n_topics, id2word=dictionary)\n",
    "coherencemodel = CoherenceModel(model=model, texts=bigram_model[preprocessed_X], dictionary=dictionary, coherence='c_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42051873442483173\n"
     ]
    }
   ],
   "source": [
    "print(coherencemodel.get_coherence())\n",
    "lda_model = models.wrappers.ldamallet.malletmodel2ldamodel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.save('./lda_model_2015.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (51092, 400)\n",
      "y_train:  (51092, 21)\n",
      "X_test:  (12773, 400)\n",
      "y_test:  (12773, 21)\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "for bowx in bow_X:\n",
    "    topics = lda_model.get_document_topics(bowx, minimum_probability=0.0)\n",
    "    topic_vec = [topics[i][1] for i in range(n_topics)]\n",
    "    inputs.append(topic_vec)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "Y = Y.reshape(-1,1)\n",
    "enc.fit(Y)\n",
    "Y = enc.transform(Y).toarray()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, Y, test_size=0.2, stratify=Y, random_state=42)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "print('X_train: ', X_train.shape)\n",
    "print('y_train: ', y_train.shape)\n",
    "print('X_test: ', X_test.shape)\n",
    "print('y_test: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.Input(shape=(n_topics)),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(21)\n",
    "    ])\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0, name='categorical_crossentropy')\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_network()\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=400, callbacks=[callback], validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./topics_classifier')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:econ] *",
   "language": "python",
   "name": "conda-env-econ-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
